{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook(s) to HTML\n",
    "\n",
    "#### Purpose: \n",
    "This tools helps to convert all the notebooks in to a single HTML page with navigation. This is especially helpful when you are explaining a single topic in multiple parts. \n",
    "\n",
    "## Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T07:21:15.881127Z",
     "start_time": "2018-12-06T07:21:15.875146Z"
    }
   },
   "outputs": [],
   "source": [
    "source_listfile = 'list_ipynb.csv'   # for now let us have the list of ipynb to be processed in same list\n",
    "template_file = \"ipy2htmlheadless_converter.tplx\"\n",
    "nbheader_template = 'nbheader_template.html'\n",
    "style_template = 'style_template.html'\n",
    "nav_script = 'navigation_script.html'\n",
    "output_filename = '20_Normal_Distribution.html'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jinja Template for Headless HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T07:21:16.321930Z",
     "start_time": "2018-12-06T07:21:15.884141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ipy2htmlheadless_converter.tplx\n"
     ]
    }
   ],
   "source": [
    "%%writefile $template_file\n",
    "\n",
    "{% extends 'full.tpl'%}\n",
    "\n",
    "{% block header %}\n",
    "{% endblock header %}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert each ipynb to headless HTML\n",
    "\n",
    "Also get the no of files and headers of each file in the process. This script already takes care of integrating attachments as base64 in the notebook\n",
    "\n",
    "<font color='red'>Ensure at least one header h1 is present as title. Currently script is not foolproof</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T07:21:17.776493Z",
     "start_time": "2018-12-06T07:21:16.330918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Binomial Distribution\n",
      "The Normal Distribution\n",
      "Sampling Distributions\n",
      "Using Normal Distribution and Z Score\n",
      "Calculating sum of dice outcomes\n"
     ]
    }
   ],
   "source": [
    "import nbformat\n",
    "import nbconvert\n",
    "import sys\n",
    "import os\n",
    "import ntpath\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "result_paths, counter, titles, bodies = [], 0, [], []\n",
    "\n",
    "# 1. Read the list of .ipynb files from list_ipynb.csv in same folder\n",
    "\n",
    "\n",
    "with open('list_ipynb.csv') as csvfile:\n",
    "    ipynb_reader = csv.reader(csvfile)\n",
    "    for each_row in ipynb_reader:\n",
    "        current_file =  each_row[0]\n",
    "        # print(current_file)\n",
    "\n",
    "        \"\"\"\n",
    "        2. For each file, create respective html \n",
    "        https://github.com/jupyter/nbconvert/issues/699\n",
    "        \"\"\"\n",
    "        with open(current_file, 'rb') as nb_file:\n",
    "            nb_contents = nb_file.read().decode('utf8')  \n",
    "\n",
    "        # Convert using the ordinary exporter\n",
    "        notebook = nbformat.reads(nb_contents, as_version=4)      \n",
    "        \n",
    "#         inname = ntpath.basename(current_file)\n",
    "#         outpath = os.path.dirname(current_file)\n",
    "#         outname = inname.split('.ipynb')[0] + '_HEADLESS.html'\n",
    "#         print(\"\\nInput:{} \\nOutput:{} \\nPath:{}\".format(inname, outname, outpath))\n",
    "#         outpath = os.path.join(outpath,outname)  # outputting in same folder as ipynb file        \n",
    "#         result_paths.append(outpath)  # to write list of htmls in a file        \n",
    "        \n",
    "        exporter = nbconvert.HTMLExporter()\n",
    "        exporter.template_file = template_file   # TEMPLATE FILE GOES HERE\n",
    "        body, res = exporter.from_notebook_node(notebook)        \n",
    "\n",
    "        # Create a list saving all image attachments to their base64 representations\n",
    "        images = []\n",
    "        for cell in notebook['cells']:\n",
    "            if 'attachments' in cell:\n",
    "                attachments = cell['attachments']\n",
    "                for filename, attachment in attachments.items():\n",
    "                    for mime, base64 in attachment.items():\n",
    "                        images.append( [f'attachment:{filename}', f'data:{mime};base64,{base64}'] )\n",
    "\n",
    "        # Fix up the HTML and write it to disk\n",
    "        for itmes in images:\n",
    "            src = itmes[0]\n",
    "            base64 = itmes[1]\n",
    "            body = body.replace(f'src=\"{src}\"', f'src=\"{base64}\"', 1)  \n",
    "\n",
    "                \n",
    "#         with open(outpath, 'w') as output_file:\n",
    "#             output_file.write(body)   \n",
    "\n",
    "        # extract first h1 text\n",
    "        soup = BeautifulSoup(body, 'html.parser')\n",
    "        title = soup.h1.contents[0]\n",
    "        titles.append(title)\n",
    "        print(title)            \n",
    "        bodies.append(body)\n",
    "        counter += 1\n",
    "        #print('{} is done'.format(each_row[0]))   \n",
    "\n",
    "# list_str = ''\n",
    "# for each_html in result_paths:\n",
    "#     list_str += each_html + '\\n'\n",
    "\n",
    "# with open('list_html.csv','w') as output_listfile:\n",
    "#     output_listfile.write(list_str) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write html with beautiful soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T07:21:18.140283Z",
     "start_time": "2018-12-06T07:21:17.778493Z"
    }
   },
   "outputs": [],
   "source": [
    "head_soup = BeautifulSoup(open(nbheader_template),\"html.parser\")\n",
    "\n",
    "style_soup = BeautifulSoup(open(style_template),\"html.parser\")\n",
    "\n",
    "base_template = \"<!DOCTYPE html><html></html>\"\n",
    "main_soup = BeautifulSoup(base_template,\"html.parser\")\n",
    "\n",
    "main_soup.html.append(head_soup)  # add nbconvert header\n",
    "\n",
    "\n",
    "# INSERT THE BODY AS IT IS\n",
    "bodies = [body.replace('<body>','').replace('</body>','') for body in bodies]  # no need of body tags\n",
    "# bodies = ['<div>Test div' + str(i+1) + '</div>' for i in range(3)] # for MWE\n",
    "body_tag = main_soup.new_tag('body')\n",
    "for i,each_body in enumerate(bodies):\n",
    "    \n",
    "    # monkey patch to change ID of first div\n",
    "    old_ID_str = '<div tabindex=\"-1\" id=\"notebook\" class=\"border-box-sizing\">'\n",
    "    new_ID_str = '<div tabindex=\"-1\" id=\"notebook_{}\" class=\"border-box-sizing\">'.format(i)    \n",
    "    div_str = each_body.replace(old_ID_str,new_ID_str)\n",
    "    # apparantly html space character is messed up    \n",
    "    nonBreakSpace = '&nbsp;' #u'\\xa0'\n",
    "    div_str = div_str.replace(nonBreakSpace,'')\n",
    "    div_soup = BeautifulSoup(div_str,'html.parser')    \n",
    "    body_tag.append(div_soup)\n",
    "    \n",
    "main_soup.html.insert(1,body_tag)    \n",
    "\n",
    "# we write before further ops because some issue in existing soup\n",
    "with open(output_filename, \"w\") as file:\n",
    "    file.write(str(main_soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T07:21:18.707657Z",
     "start_time": "2018-12-06T07:21:18.142283Z"
    }
   },
   "outputs": [],
   "source": [
    "# set up the display settings and the navigation part\n",
    "\n",
    "# we read back caz some issue continuing soup from above\n",
    "soup = BeautifulSoup(open(output_filename),\"html.parser\")  \n",
    "\n",
    "\n",
    "# display first page\n",
    "first_page_soup = soup.find('div', {'id': 'notebook_0'})\n",
    "first_page_soup.attrs['style'] ='display:block;'\n",
    "\n",
    "# hide other pages\n",
    "for i in range(1,counter): # from second page...\n",
    "    rest_page_soup = soup.find('div', {'id': 'notebook_{}'.format(i)})\n",
    "    rest_page_soup.attrs['style'] ='display:none;'\n",
    "    \n",
    "# navigation tags\n",
    "nav_tag = soup.new_tag('nav') \n",
    "ul_tag = soup.new_tag('ul', class_='nav')\n",
    "soup.body.insert(0,nav_tag)\n",
    "soup.body.nav.insert(0,ul_tag)\n",
    "    \n",
    "for i in reversed(range(counter)):\n",
    "    li_tag = soup.new_tag('li')\n",
    "    a_tag = soup.new_tag('a', href='#', onclick='divVisibility({});'.format(i))\n",
    "    a_tag.string = 'Part ' + str(i+1) + ' : ' + titles[i]\n",
    "    li_tag.append(a_tag)\n",
    "    soup.body.nav.ul.insert(0,li_tag)\n",
    "    \n",
    "# script tag for navigation\n",
    "# script tag for navigation\n",
    "divsID_js = \"var divsID = [\"\n",
    "for i in range(counter):\n",
    "    divsID_js += \"'notebook_{}' , \".format(i)\n",
    "divsID_js += '];'\n",
    "divsID_js = '<script>' + divsID_js + '</script>'\n",
    "prescript_soup = BeautifulSoup(divsID_js, 'html.parser')\n",
    "soup.body.append(prescript_soup)  # add divsId array\n",
    "\n",
    "script_soup = BeautifulSoup(open(nav_script),\"html.parser\")  \n",
    "soup.body.append(script_soup)  # add nbconver header\n",
    "\n",
    "# update the changes\n",
    "with open(output_filename, \"w\") as file:\n",
    "    file.write(str(soup.prettify()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T07:21:18.763554Z",
     "start_time": "2018-12-06T07:21:18.707657Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.startfile(output_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
